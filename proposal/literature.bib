@inproceedings{Representative_Benchmark,
	author = {Saleem, Muhammad and Sz\'{a}rnyas, G\'{a}bor and Conrads, Felix and Bukhari, Syed Ahmad Chan and Mehmood, Qaiser and Ngonga Ngomo, Axel-Cyrille},
	title = {How Representative Is a SPARQL Benchmark? An Analysis of RDF Triplestore Benchmarks},
	year = {2019},
	isbn = {9781450366748},
	publisher = {Association for Computing Machinery},
	address = {New York, NY, USA},
	url = {https://doi.org/10.1145/3308558.3313556},
	doi = {10.1145/3308558.3313556},
	abstract = {Triplestores are data management systems for storing and querying RDF data. Over recent
	years, various benchmarks have been proposed to assess the performance of triplestores
	across different performance measures. However, choosing the most suitable benchmark
	for evaluating triplestores in practical settings is not a trivial task. This is because
	triplestores experience varying workloads when deployed in real applications. We address
	the problem of determining an appropriate benchmark for a given real-life workload
	by providing a fine-grained comparative analysis of existing triplestore benchmarks.
	In particular, we analyze the data and queries provided with the existing triplestore
	benchmarks in addition to several real-world datasets. Furthermore, we measure the
	correlation between the query execution time and various SPARQL query features and
	rank those features based on their significance levels. Our experiments reveal several
	interesting insights about the design of such benchmarks. With this fine-grained evaluation,
	we aim to support the design and implementation of more diverse benchmarks. Application
	developers can use our result to analyze their data and queries and choose a data
	management system.},
	booktitle = {The World Wide Web Conference},
	pages = {1623–1633},
	numpages = {11},
	location = {San Francisco, CA, USA},
	series = {WWW '19}
}

@inproceedings{IGUANA,
	author    = {Felix Conrads and
	Jens Lehmann and
	Muhammad Saleem and
	Mohamed Morsey and
	Axel{-}Cyrille Ngonga Ngomo},
	editor    = {Claudia d'Amato and
	Miriam Fern{\'{a}}ndez and
	Valentina A. M. Tamma and
	Freddy L{\'{e}}cu{\'{e}} and
	Philippe Cudr{\'{e}}{-}Mauroux and
	Juan F. Sequeda and
	Christoph Lange and
	Jeff Heflin},
	title     = {Iguana: {A} Generic Framework for Benchmarking the Read-Write Performance
	of Triple Stores},
	booktitle = {The Semantic Web - {ISWC} 2017 - 16th International Semantic Web Conference,
	Vienna, Austria, October 21-25, 2017, Proceedings, Part {II}},
	series    = {Lecture Notes in Computer Science},
	volume    = {10588},
	pages     = {48--65},
	publisher = {Springer},
	year      = {2017},
	url       = {https://doi.org/10.1007/978-3-319-68204-4\_5},
	doi       = {10.1007/978-3-319-68204-4\_5},
	timestamp = {Tue, 07 Sep 2021 13:48:23 +0200},
	biburl    = {https://dblp.org/rec/conf/semweb/ConradsLSMN17.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org}
}

@InProceedings{tentris,
	author = {Bigerl, Alexander and Conrads, Felix and Behning, Charlotte and Sherif, Mohamed Ahmed and Saleem, Muhammad and Ngonga Ngomo, Axel-Cyrille},
	booktitle = {The Semantic Web -- ISWC 2020},
	publisher = {Springer International Publishing},
	title = { {T}entris -- {A} {T}ensor-{B}ased {T}riple {S}tore},
	pages = {56--73},
	url = {https://papers.dice-research.org/2020/ISWC_Tentris/iswc2020_tentris_public.pdf},
	year = 2020,
	isbn = {978-3-030-62419-4}
}

@paper{Usage-Centric-Benchmarking,
	author = {Mohamed Morsey and Jens Lehmann and Sören Auer and Axel-Cyrille Ngonga Ngomo},
	title = {Usage-Centric Benchmarking of RDF Triple Stores},
	conference = {AAAI Conference on Artificial Intelligence},
	year = {2012},
	keywords = {Triple Stores; Benchmark; RDF; SPARQL},
	abstract = {A central component in many applications is the underlying data management layer. In Data-Web applications, the central component of this layer is the triple store. It is thus evident that finding the most adequate store for the application to develop is of crucial importance for individual projects as well as for data integration on the Data Web in general. In this paper, we propose a generic benchmark creation procedure for SPARQL, which we apply to the DBpedia knowledge base. In contrast to previous approaches, our benchmark is based on queries that were actually issued by humans and applications against existing RDF data not resembling a relational schema. In addition, our approach does not only take the query string but also the features of the queries into consideration during the benchmark generation process. Our generic procedure for benchmark creation is based on query-log mining, SPARQL feature analysis and clustering. After presenting the method underlying our benchmark generation algorithm, we use the generated benchmark to compare the popular triple store implementations Virtuoso, Sesame, Jena-TDB, and BigOWLIM.},
	
	url = {https://www.aaai.org/ocs/index.php/AAAI/AAAI12/paper/view/5168/5384}
}

@inproceedings{yet-another-benchmark,
	author    = {Martin Voigt and
	Annett Mitschick and
	Jonas Schulz},
	editor    = {Annett Mitschick and
	Fernando Loizides and
	Livia Predoiu and
	Andreas N{\"{u}}rnberger and
	Seamus Ross},
	title     = {Yet Another Triple Store Benchmark? Practical Experiences with Real-World
	Data},
	booktitle = {Proceedings of the 2nd International Workshop on Semantic Digital
	Archives, Paphos, Cyprus, September 27, 2012},
	series    = {{CEUR} Workshop Proceedings},
	volume    = {912},
	pages     = {85--94},
	publisher = {CEUR-WS.org},
	year      = {2012},
	url       = {http://ceur-ws.org/Vol-912/paper7.pdf},
	timestamp = {Wed, 12 Feb 2020 16:44:15 +0100},
	biburl    = {https://dblp.org/rec/conf/ercimdl/VoigtMS12.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{benchmark-bio-data,
	author    = {Vladimir Mironov and
	Nirmala Seethappan and
	Ward Blond{\'{e}} and
	Erick Antezana and
	Bjorn Lindi and
	Martin Kuiper},
	editor    = {Albert Burger and
	M. Scott Marshall and
	Paolo Romano and
	Adrian Paschke and
	Andrea Splendiani},
	title     = {Benchmarking Triple Stores with Biological Data},
	booktitle = {Proceedings of the Workshop on Semantic Web Applications and Tools
	for Life Sciences, Berlin, Germany, December 10, 2010},
	series    = {{CEUR} Workshop Proceedings},
	volume    = {698},
	publisher = {CEUR-WS.org},
	year      = {2010},
	url       = {http://ceur-ws.org/Vol-698/paper12.pdf},
	timestamp = {Wed, 12 Feb 2020 16:44:20 +0100},
	biburl    = {https://dblp.org/rec/conf/swat4ls/MironovSBALK10.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org}
}

@InProceedings{10.1007/978-3-319-25007-6_4,
	author="Saleem, Muhammad
	and Mehmood, Qaiser
	and Ngonga Ngomo, Axel-Cyrille",
	editor="Arenas, Marcelo
	and Corcho, Oscar
	and Simperl, Elena
	and Strohmaier, Markus
	and d'Aquin, Mathieu
	and Srinivas, Kavitha
	and Groth, Paul
	and Dumontier, Michel
	and Heflin, Jeff
	and Thirunarayan, Krishnaprasad
	and Thirunarayan, Krishnaprasad
	and Staab, Steffen",
	title="FEASIBLE: A Feature-Based SPARQL Benchmark Generation Framework",
	booktitle="The Semantic Web - ISWC 2015",
	year="2015",
	publisher="Springer International Publishing",
	address="Cham",
	pages="52--69",
	abstract="Benchmarking is indispensable when aiming to assess technologies with respect to their suitability for given tasks. While several benchmarks and benchmark generation frameworks have been developed to evaluate triple stores, they mostly provide a one-fits-all solution to the benchmarking problem. This approach to benchmarking is however unsuitable to evaluate the performance of a triple store for a given application with particular requirements. We address this drawback by presenting FEASIBLE, an automatic approach for the generation of benchmarks out of the query history of applications, i.e., query logs. The generation is achieved by selecting prototypical queries of a user-defined size from the input set of queries. We evaluate our approach on two query logs and show that the benchmarks it generates are accurate approximations of the input query logs. Moreover, we compare four different triple stores with benchmarks generated using our approach and show that they behave differently based on the data they contain and the types of queries posed. Our results suggest that FEASIBLE generates better sample queries than the state of the art. In addition, the better query selection and the larger set of query types used lead to triple store rankings which partly differ from the rankings generated by previous works.",
	isbn="978-3-319-25007-6"
}

    @inproceedings{10.1145/3209950.3209955,
	author = {Raasveldt, Mark and Holanda, Pedro and Gubner, Tim and M\"{u}hleisen, Hannes},
	title = {Fair Benchmarking Considered Difficult: Common Pitfalls In Database Performance Testing},
	year = {2018},
	isbn = {9781450358262},
	publisher = {Association for Computing Machinery},
	address = {New York, NY, USA},
	url = {https://doi.org/10.1145/3209950.3209955},
	doi = {10.1145/3209950.3209955},
	abstract = {Performance benchmarking is one of the most commonly used methods for comparing different systems or algorithms, both in scientific literature and in industrial publications. While performance measurements might seem objective on the surface, there are many different ways to influence benchmark results to favor one system over the other, either by accident or on purpose. In this paper, we perform a study of the common pitfalls in DBMS performance comparisons, and give advice on how they can be spotted and avoided so a fair performance comparison between systems can be made. We illustrate the common pitfalls with a series of mock benchmarks, which show large differences in performance where none should be present.},
	booktitle = {Proceedings of the Workshop on Testing Database Systems},
	articleno = {2},
	numpages = {6},
	keywords = {Performance Evaluation, Benchmarking},
	location = {Houston, TX, USA},
	series = {DBTest'18}
}



