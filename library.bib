
@inproceedings{alucDiversifiedStressTesting2014,
  title = {Diversified {{Stress Testing}} of {{RDF Data Management Systems}}},
  booktitle = {The {{Semantic Web}} – {{ISWC}} 2014},
  author = {Aluç, Güneş and Hartig, Olaf and Özsu, M. Tamer and Daudjee, Khuzaima},
  editor = {Mika, Peter and Tudorache, Tania and Bernstein, Abraham and Welty, Chris and Knoblock, Craig and Vrandečić, Denny and Groth, Paul and Noy, Natasha and Janowicz, Krzysztof and Goble, Carole},
  date = {2014},
  series = {Lecture {{Notes}} in {{Computer Science}}},
  pages = {197--212},
  publisher = {{Springer International Publishing}},
  location = {{Cham}},
  doi = {10.1007/978-3-319-11964-9_13},
  url = {https://link.springer.com/chapter/10.1007%2F978-3-319-11964-9_13},
  abstract = {The Resource Description Framework (RDF) is a standard for conceptually describing data on the Web, and SPARQL is the query language for RDF. As RDF data continue to be published across heterogeneous domains and integrated at Web-scale such as in the Linked Open Data (LOD) cloud, RDF data management systems are being exposed to queries that are far more diverse and workloads that are far more varied. The first contribution of our work is an in-depth experimental analysis that shows existing SPARQL benchmarks are not suitable for testing systems for diverse queries and varied workloads. To address these shortcomings, our second contribution is the Waterloo SPARQL Diversity Test Suite (WatDiv) that provides stress testing tools for RDF data management systems. Using WatDiv, we have been able to reveal issues with existing systems that went unnoticed in evaluations using earlier benchmarks. Specifically, our experiments with five popular RDF data management systems show that they cannot deliver good performance uniformly across workloads. For some queries, there can be as much as five orders of magnitude difference between the query execution time of the fastest and the slowest system while the fastest system on one query may unexpectedly time out on another query. By performing a detailed analysis, we pinpoint these problems to specific types of queries and workloads.},
  isbn = {978-3-319-11964-9},
  langid = {english},
  keywords = {benchmarking,RDF,SPARQL,systems,workload diversity},
  file = {D\:\\Zotero\\storage\\42ZQPKNQ\\Aluç et al. - 2014 - Diversified Stress Testing of RDF Data Management .pdf}
}

@online{archiveddocsRepositoryPatternHttps,
  title = {The {{Repository Pattern}}, {{https://docs.microsoft.com/en-us/previous-versions/msp-n-p/ff649690(v=pandp.10)}}},
  author = {Archiveddocs},
  url = {https://docs.microsoft.com/en-us/previous-versions/msp-n-p/ff649690(v=pandp.10)},
  urldate = {2022-03-16},
  langid = {american},
  file = {D\:\\Zotero\\storage\\2FGYGA8Y\\ff649690(v=pandp.html}
}

@online{bansalDAOVsRepository2020,
  title = {{{DAO}} vs {{Repository Patterns}} | {{Baeldung}}, {{https://www.baeldung.com/java-dao-vs-repository}}},
  author = {Bansal, Anshul},
  date = {2020-09-10T05:51:31+00:00},
  url = {https://www.baeldung.com/java-dao-vs-repository},
  urldate = {2022-03-17},
  abstract = {Understand the difference between the DAO and Repository patterns with a Java example.},
  langid = {american},
  file = {D\:\\Zotero\\storage\\C3IYV4TZ\\java-dao-vs-repository.html}
}

@incollection{bigerlTentrisTensorBasedTriple2020,
  title = {Tentris – {{A Tensor-Based Triple Store}}},
  booktitle = {The {{Semantic Web}} – {{ISWC}} 2020},
  author = {Bigerl, Alexander and Conrads, Felix and Behning, Charlotte and Sherif, Mohamed Ahmed and Saleem, Muhammad and Ngonga Ngomo, Axel-Cyrille},
  editor = {Pan, Jeff Z. and Tamma, Valentina and d’ Amato, Claudia and Janowicz, Krzysztof and Fu, Bo and Polleres, Axel and Seneviratne, Oshani and Kagal, Lalana},
  options = {useprefix=true},
  date = {2020},
  series = {Lecture {{Notes}} in {{Computer Science}}},
  volume = {12506},
  pages = {56--73},
  publisher = {{Springer International Publishing}},
  location = {{Cham}},
  doi = {10.1007/978-3-030-62419-4_4},
  url = {https://link.springer.com/10.1007/978-3-030-62419-4_4},
  urldate = {2021-11-22},
  abstract = {The number and size of RDF knowledge graphs grows continuously. Efficient storage solutions for these graphs are indispensable for their use in real applications. We present such a storage solution dubbed TENTRIS. Our solution represents RDF knowledge graphs as sparse order-3 tensors using a novel data structure, which we dub hypertrie. It then uses tensor algebra to carry out SPARQL queries by mapping SPARQL operations to Einstein summation. By being able to compute Einstein summations efficiently, TENTRIS outperforms the commercial and open-source RDF storage solutions evaluated in our experiments by at least 1.8 times with respect to the average number of queries it can serve per second on three datasets of up to 1 billion triples. Our code, evaluation setup, results, supplementary material and the datasets are provided at https://tentris.dice-research.org/iswc2020.},
  isbn = {978-3-030-62418-7 978-3-030-62419-4},
  langid = {english},
  file = {D\:\\Zotero\\storage\\SDRTI6NN\\Bigerl et al. - 2020 - Tentris – A Tensor-Based Triple Store.pdf}
}

@incollection{conradsIguanaGenericFramework2017,
  title = {Iguana: {{A Generic Framework}} for {{Benchmarking}} the {{Read-Write Performance}} of {{Triple Stores}}},
  shorttitle = {Iguana},
  booktitle = {The {{Semantic Web}} – {{ISWC}} 2017},
  author = {Conrads, Felix and Lehmann, Jens and Saleem, Muhammad and Morsey, Mohamed and Ngonga Ngomo, Axel-Cyrille},
  editor = {d' Amato, Claudia and Fernandez, Miriam and Tamma, Valentina and Lecue, Freddy and Cudré-Mauroux, Philippe and Sequeda, Juan and Lange, Christoph and Heflin, Jeff},
  options = {useprefix=true},
  date = {2017},
  series = {Lecture {{Notes}} in {{Computer Science}}},
  volume = {10588},
  pages = {48--65},
  publisher = {{Springer International Publishing}},
  location = {{Cham}},
  doi = {10.1007/978-3-319-68204-4_5},
  url = {http://link.springer.com/10.1007/978-3-319-68204-4_5},
  urldate = {2021-11-22},
  abstract = {The performance of triples stores is crucial for applications driven by RDF. Several benchmarks have been proposed that assess the performance of triple stores. However, no integrated benchmarkindependent execution framework for these benchmarks has yet been provided. We propose a novel SPARQL benchmark execution framework called Iguana. Our framework complements benchmarks by providing an execution environment which can measure the performance of triple stores during data loading, data updates as well as under different loads and parallel requests. Moreover, it allows a uniform comparison of results on different benchmarks. We execute the FEASIBLE and DBPSB benchmarks using the Iguana framework and measure the performance of popular triple stores under updates and parallel user requests. We compare our results (See https://doi.org/10.6084/m9.figshare.c.3767501.v1) with state-of-the-art benchmarking results and show that our benchmark execution framework can unveil new insights pertaining to the performance of triple stores.},
  isbn = {978-3-319-68203-7 978-3-319-68204-4},
  langid = {english},
  file = {D\:\\Zotero\\storage\\S2JMR2RR\\Conrads et al. - 2017 - Iguana A Generic Framework for Benchmarking the R.pdf}
}

@unpublished{dragoniMicroservicesYesterdayToday2017,
  title = {Microservices: Yesterday, Today, and Tomorrow},
  shorttitle = {Microservices},
  author = {Dragoni, Nicola and Giallorenzo, Saverio and Lafuente, Alberto Lluch and Mazzara, Manuel and Montesi, Fabrizio and Mustafin, Ruslan and Safina, Larisa},
  date = {2017-04-20},
  eprint = {1606.04036},
  eprinttype = {arxiv},
  primaryclass = {cs},
  url = {http://arxiv.org/abs/1606.04036},
  urldate = {2022-01-07},
  abstract = {Microservices is an architectural style inspired by service-oriented computing that has recently started gaining popularity. Before presenting the current state-of-the-art in the field, this chapter reviews the history of software architecture, the reasons that led to the diffusion of objects and services first, and microservices later. Finally, open problems and future challenges are introduced. This survey primarily addresses newcomers to the discipline, while offering an academic viewpoint on the topic. In addition, we investigate some practical issues and point out some potential solutions.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Software Engineering},
  file = {D\:\\Zotero\\storage\\QPHKBNHZ\\Dragoni et al. - 2017 - Microservices yesterday, today, and tomorrow.pdf}
}

@book{fowlerPatternsEnterpriseApplication2003,
  title = {Patterns of Enterprise Application Architecture},
  author = {Fowler, Martin},
  date = {2003},
  series = {The {{Addison-Wesley}} Signature Series},
  publisher = {{Addison-Wesley}},
  location = {{Boston}},
  isbn = {978-0-321-12742-6},
  pagetotal = {533},
  keywords = {Application software,Business,Computer architecture,Data processing,Development,System design}
}

@book{fowlerRefactoringImprovingDesign2019a,
  title = {Refactoring: Improving the Design of Existing Code},
  shorttitle = {Refactoring},
  author = {Fowler, Martin},
  date = {2019},
  series = {Addison-{{Wesley}} Signature Series},
  edition = {Second edition},
  publisher = {{Addison-Wesley}},
  location = {{Boston}},
  isbn = {978-0-13-475759-9},
  pagetotal = {418},
  keywords = {Object-oriented programming (Computer science),Software reengineering},
  annotation = {OCLC: on1064139838}
}

@online{fowlerRepositoryPatternHttps,
  title = {Repository {{Pattern}}, {{https://martinfowler.com/eaaCatalog/repository.html}}},
  author = {Fowler, Martin},
  url = {https://martinfowler.com/eaaCatalog/repository.html},
  urldate = {2022-03-17},
  file = {D\:\\Zotero\\storage\\I7Y24HCJ\\repository.html}
}

@online{gibbsMicrobenchmarksVsMacrobenchmarks,
  title = {Microbenchmarks vs {{Macrobenchmarks}}; {{Retrieved May}} 6, 2022,  {{https://engineering.appfolio.com/appfolio-engineering/2019/1/7/microbenchmarks-vs-macrobenchmarks-ie-whats-a-microbenchmark}}},
  shorttitle = {Microbenchmarks vs {{Macrobenchmarks}} (i.e. {{What}}'s a {{Microbenchmark}}?},
  author = {Gibbs, Noah},
  url = {https://engineering.appfolio.com/appfolio-engineering/2019/1/7/microbenchmarks-vs-macrobenchmarks-ie-whats-a-microbenchmark},
  urldate = {2022-06-06},
  abstract = {I’ve mentioned a few times recently that something is a “microbenchmark.” What does that mean? Is it good or bad?  Let’s talk about that. Along the way, we’ll talk about benchmarks that are not microbenchmarks and how to pick a scale/size for a specific benchmark.  I talk about this because I  write},
  langid = {american},
  organization = {{AppFolio Engineering}},
  file = {D\:\\Zotero\\storage\\PC4MAK9Q\\microbenchmarks-vs-macrobenchmarks-ie-whats-a-microbenchmark.html}
}

@online{GraphDBDownloadsResources,
  title = {{{GraphDB Downloads}} and {{Resources}}},
  url = {https://graphdb.ontotext.com/},
  urldate = {2022-06-05},
  file = {D\:\\Zotero\\storage\\WMTYWVER\\graphdb.ontotext.com.html}
}

@article{guoLUBMBenchmarkOWL2005,
  title = {{{LUBM}}: {{A}} Benchmark for {{OWL}} Knowledge Base Systems},
  shorttitle = {{{LUBM}}},
  author = {Guo, Yuanbo and Pan, Zhengxiang and Heflin, Jeff},
  date = {2005-10-01},
  journaltitle = {Journal of Web Semantics},
  shortjournal = {Journal of Web Semantics},
  series = {Selcted {{Papers}} from the {{International Semantic Web Conference}}, 2004},
  volume = {3},
  number = {2},
  pages = {158--182},
  issn = {1570-8268},
  doi = {10.1016/j.websem.2005.06.005},
  url = {https://www.sciencedirect.com/science/article/pii/S1570826805000132},
  urldate = {2021-11-23},
  abstract = {We describe our method for benchmarking Semantic Web knowledge base systems with respect to use in large OWL applications. We present the Lehigh University Benchmark (LUBM) as an example of how to design such benchmarks. The LUBM features an ontology for the university domain, synthetic OWL data scalable to an arbitrary size, 14 extensional queries representing a variety of properties, and several performance metrics. The LUBM can be used to evaluate systems with different reasoning capabilities and storage mechanisms. We demonstrate this with an evaluation of two memory-based systems and two systems with persistent storage.},
  langid = {english},
  keywords = {Evaluation,Knowledge base system,Lehigh University Benchmark,Semantic Web},
  file = {D\:\\Zotero\\storage\\R9IBBGW5\\Guo et al. - 2005 - LUBM A benchmark for OWL knowledge base systems.pdf;D\:\\Zotero\\storage\\XXN6FTVJ\\S1570826805000132.html}
}

@online{harrisSPARQLQueryLanguage,
  title = {{{SPARQL}} 1.1 {{Query Language}}, {{Retrieved Dec}} 12, 2021, {{https://www.w3.org/TR/sparql11-query/}}},
  author = {Harris, Steve and Seaborne, Andy and Prud'hommeaux, Eric},
  url = {https://www.w3.org/TR/sparql11-query/},
  urldate = {2021-12-21},
  file = {D\:\\Zotero\\storage\\FSLU5U8L\\sparql11-query.html}
}

@article{HashingHypertrieSpace2018,
  title = {Hashing the {{Hypertrie}}: {{Space-}} and {{Time-Efficient Indexing}} for {{SPARQL}} in {{Tensors}}},
  date = {2018},
  pages = {10},
  abstract = {Time-efficient solutions for querying RDF knowledge graphs depend on indexing structures with low response times to answer SPARQL queries rapidly. Hypertries—an indexing structure recently developed for tensor-based triple stores—have achieved significant runtime improvements over several mainstream storage solutions for RDF knowledge graphs. However, the space footprint of this novel data structure is still often larger than that of many mainstream solutions. In this work, we show how to reduce the memory footprint of hypertries and thereby further speed up query processing in hypertrie-based RDF storage solutions. Our approach relies on three strategies: (1) the elimination of duplicates via hashing, (2) the compression of non-branching paths, and (3) the storage of single-entry leaf nodes in their parent nodes. We evaluate these strategies by comparing them with baseline hypertries as well as popular triple stores such as Virtuoso, Fuseki, GraphDB, Blazegraph and gStore. We rely on four datasets/benchmark generators in our evaluation: SWDF, DBpedia, WatDiv, and WikiData. Our results suggest that our modifications significantly reduce the memory footprint of hypertries by up to 70\% while leading to a relative improvement of up to 39\% with respect to average Queries per Second and up to 740\% with respect to Query Mixes per Hour.},
  langid = {english},
  file = {D\:\\Zotero\\storage\\L3W2XAVC\\2018 - Hashing the Hypertrie Space- and Time-Efficient I.pdf}
}

@book{hitzlerSemanticWebGrundlagen2008,
  title = {Semantic Web: Grundlagen},
  shorttitle = {Semantic Web},
  editor = {Hitzler, Pascal and Krötzsch, Markus and Rudolph, Sebastian and Sure-Vetter, York},
  date = {2008},
  series = {eXamen.press},
  edition = {1. Aufl},
  publisher = {{Springer}},
  location = {{Berlin Heidelberg}},
  isbn = {978-3-540-33993-9},
  langid = {german},
  pagetotal = {277},
  file = {D\:\\Zotero\\storage\\TD7ZIVA6\\Hitzler et al. - 2008 - Semantic Web Grundlagen.pdf}
}

@article{hoganKnowledgeGraphs2021,
  title = {Knowledge {{Graphs}}},
  author = {Hogan, Aidan and Blomqvist, Eva and Cochez, Michael and D’amato, Claudia and Melo, Gerard De and Gutierrez, Claudio and Kirrane, Sabrina and Gayo, José Emilio Labra and Navigli, Roberto and Neumaier, Sebastian and Ngomo, Axel-Cyrille Ngonga and Polleres, Axel and Rashid, Sabbir M. and Rula, Anisa and Schmelzeisen, Lukas and Sequeda, Juan and Staab, Steffen and Zimmermann, Antoine},
  date = {2021-07-02},
  journaltitle = {ACM Computing Surveys},
  shortjournal = {ACM Comput. Surv.},
  volume = {54},
  number = {4},
  pages = {71:1--71:37},
  issn = {0360-0300},
  doi = {10.1145/3447772},
  url = {https://doi.org/10.1145/3447772},
  urldate = {2021-12-20},
  abstract = {In this article, we provide a comprehensive introduction to knowledge graphs, which have recently garnered significant attention from both industry and academia in scenarios that require exploiting diverse, dynamic, large-scale collections of data. After some opening remarks, we motivate and contrast various graph-based data models, as well as languages used to query and validate knowledge graphs. We explain how knowledge can be represented and extracted using a combination of deductive and inductive techniques. We conclude with high-level future research directions for knowledge graphs.},
  keywords = {embeddings,graph algorithms,graph databases,graph neural networks,graph query languages,Knowledge graphs,ontologies,rule mining,shapes},
  file = {D\:\\Zotero\\storage\\ZBB7QR25\\Hogan et al. - 2021 - Knowledge Graphs.pdf}
}

@online{IguanaDocsConfigurationa,
  title = {Iguana {{Docs}} - {{Configuration}}, {{Retrieved Feb}} 15, 2022, {{http://iguana-benchmark.eu/docs/3.2/usage/configuration/}}},
  url = {http://iguana-benchmark.eu/docs/3.2/usage/configuration/},
  urldate = {2022-06-09},
  file = {D\:\\Zotero\\storage\\WUC75NGK\\configuration.html}
}

@online{IguanaDocsMetrics,
  title = {Iguana {{Docs}} - {{Metrics}}, {{Retrieved Nov}} 23, 2021, {{http://iguana-benchmark.eu/docs/3.2/usage/metrics/}}},
  shorttitle = {{{http://iguana-benchmark.eu/docs/3.3/usage/metrics/}}},
  url = {http://iguana-benchmark.eu/docs/3.3/usage/metrics/},
  urldate = {2021-11-23},
  file = {D\:\\Zotero\\storage\\SN8J9MXM\\metrics.html}
}

@online{MicroservicesRetrievedJan,
  title = {Microservices, {{Retrieved Jan}} 7, 2022, {{https://martinfowler.com/articles/microservices.html}}},
  url = {https://martinfowler.com/articles/microservices.html},
  urldate = {2022-01-07},
  abstract = {Defining the microservices architectural style by describing their nine common characteristics},
  organization = {{martinfowler.com}},
  file = {D\:\\Zotero\\storage\\UJPJCPML\\microservices.html}
}

@inproceedings{morseyDBpediaSPARQLBenchmark2011,
  title = {{{DBpedia SPARQL Benchmark}} – {{Performance Assessment}} with {{Real Queries}} on {{Real Data}}},
  booktitle = {The {{Semantic Web}} – {{ISWC}} 2011},
  author = {Morsey, Mohamed and Lehmann, Jens and Auer, Sören and Ngonga Ngomo, Axel-Cyrille},
  editor = {Aroyo, Lora and Welty, Chris and Alani, Harith and Taylor, Jamie and Bernstein, Abraham and Kagal, Lalana and Noy, Natasha and Blomqvist, Eva},
  date = {2011},
  series = {Lecture {{Notes}} in {{Computer Science}}},
  pages = {454--469},
  publisher = {{Springer}},
  location = {{Berlin, Heidelberg}},
  doi = {10.1007/978-3-642-25073-6_29},
  abstract = {Triple stores are the backbone of increasingly many Data Web applications. It is thus evident that the performance of those stores is mission critical for individual projects as well as for data integration on the Data Web in general. Consequently, it is of central importance during the implementation of any of these applications to have a clear picture of the weaknesses and strengths of current triple store implementations. In this paper, we propose a generic SPARQL benchmark creation procedure, which we apply to the DBpedia knowledge base. Previous approaches often compared relational and triple stores and, thus, settled on measuring performance against a relational database which had been converted to RDF by using SQL-like queries. In contrast to those approaches, our benchmark is based on queries that were actually issued by humans and applications against existing RDF data not resembling a relational schema. Our generic procedure for benchmark creation is based on query-log mining, clustering and SPARQL feature analysis. We argue that a pure SPARQL benchmark is more useful to compare existing triple stores and provide results for the popular triple store implementations Virtuoso, Sesame, Jena-TDB, and BigOWLIM. The subsequent comparison of our results with other benchmark results indicates that the performance of triple stores is by far less homogeneous than suggested by previous benchmarks.},
  isbn = {978-3-642-25073-6},
  langid = {english},
  keywords = {Dataset Size,Resource Description Framework,Resource Description Framework Data,SPARQL Query,Triple Pattern},
  file = {D\:\\Zotero\\storage\\JRLY6BJA\\Morsey et al. - 2011 - DBpedia SPARQL Benchmark – Performance Assessment .pdf}
}

@article{morseyUsageCentricBenchmarkingRDF,
  title = {Usage-{{Centric Benchmarking}} of {{RDF Triple Stores}}},
  author = {Morsey, Mohamed and Lehmann, Jens and Auer, Soren and Ngomo, Axel-Cyrille Ngonga},
  pages = {7},
  langid = {english},
  file = {D\:\\Zotero\\storage\\C2CRZHUV\\Morsey et al. - Usage-Centric Benchmarking of RDF Triple Stores.pdf}
}

@inproceedings{raasveldtFairBenchmarkingConsidered2018,
  title = {Fair {{Benchmarking Considered Difficult}}: {{Common Pitfalls In Database Performance Testing}}},
  shorttitle = {Fair {{Benchmarking Considered Difficult}}},
  booktitle = {Proceedings of the {{Workshop}} on {{Testing Database Systems}}},
  author = {Raasveldt, Mark and Holanda, Pedro and Gubner, Tim and Mühleisen, Hannes},
  date = {2018-06-15},
  pages = {1--6},
  publisher = {{ACM}},
  location = {{Houston TX USA}},
  doi = {10.1145/3209950.3209955},
  url = {https://dl.acm.org/doi/10.1145/3209950.3209955},
  urldate = {2021-11-22},
  abstract = {Performance benchmarking is one of the most commonly used methods for comparing different systems or algorithms, both in scientific literature and in industrial publications. While performance measurements might seem objective on the surface, there are many different ways to influence benchmark results to favor one system over the other, either by accident or on purpose. In this paper, we perform a study of the common pitfalls in DBMS performance comparisons, and give advice on how they can be spotted and avoided so a fair performance comparison between systems can be made. We illustrate the common pitfalls with a series of mock benchmarks, which show large differences in performance where none should be present.},
  eventtitle = {{{SIGMOD}}/{{PODS}} '18: {{International Conference}} on {{Management}} of {{Data}}},
  isbn = {978-1-4503-5826-2},
  langid = {english},
  file = {D\:\\Zotero\\storage\\H885JGPR\\Raasveldt et al. - 2018 - Fair Benchmarking Considered Difficult Common Pit.pdf}
}

@online{RabbitMQWebsiteHttps,
  title = {{{RabbitMQ}} Website, {{https://www.rabbitmq.com/}}},
  url = {https://www.rabbitmq.com/},
  urldate = {2022-01-31},
  file = {D\:\\Zotero\\storage\\6RXAX3Y9\\www.rabbitmq.com.html}
}

@online{RDFConceptsAbstract,
  title = {{{RDF}} 1.1 {{Concepts}} and {{Abstract Syntax}}, {{Retrieved Jan}} 28, 2022, {{https://www.w3.org/TR/rdf11-concepts/}}},
  url = {https://www.w3.org/TR/rdf11-concepts/},
  urldate = {2022-01-28},
  file = {D\:\\Zotero\\storage\\25CUXPDK\\rdf11-concepts.html}
}

@online{RDFTurtle,
  title = {{{RDF}} 1.1 {{Turtle}}, {{Retrieved Jan}} 28, 2022, {{https://www.w3.org/TR/2014/REC-turtle-20140225/}}},
  url = {https://www.w3.org/TR/2014/REC-turtle-20140225/},
  urldate = {2022-01-28},
  file = {D\:\\Zotero\\storage\\PI5HJRMM\\REC-turtle-20140225.html}
}

@article{roderHOBBITPlatformBenchmarking,
  title = {{{HOBBIT}}: {{A}} Platform for Benchmarking {{Big Linked Data}}},
  author = {Roder, Michael and Kuchelev, Denis and Ngomo, Axel-Cyrille Ngonga},
  pages = {21},
  abstract = {An increasing number of solutions aim to support the steady increase of the number of requirements and requests for Linked Data at scale. This plethora of solutions leads to a growing need for objective means that facilitate the selection of adequate solutions for particular use cases. We hence present HOBBIT, a distributed benchmarking platform designed for the unified execution of benchmarks for Linked Data solutions. The HOBBIT benchmarking platform is based on the FAIR principles and is the first benchmarking platform able to scale up to benchmarking real-world scenarios for Big Linked Data solutions. Our online instance of the platform has more than 300 registered users and offers more than 40 benchmarks. It has been used in eleven benchmarking challenges and for more than 13000 experiments. We give an overview of the results achieved during 2 of these challenges and point to some of the novel insights that were gained from the results of the platform. HOBBIT is open-source and available at http://github.com/hobbit-project.},
  langid = {english},
  file = {D\:\\Zotero\\storage\\7G45L7ZM\\Roder et al. - HOBBIT A platform for benchmarking Big Linked Dat.pdf}
}

@online{rusherTriplestoresRetrievedFeb,
  title = {Triplestores, {{Retrieved Feb}} 10, 2022, {{https://www.w3.org/2001/sw/Europe/events/20031113-storage/positions/rusher.html}}},
  author = {Rusher, Jack},
  url = {https://www.w3.org/2001/sw/Europe/events/20031113-storage/positions/rusher.html},
  urldate = {2022-06-05},
  file = {D\:\\Zotero\\storage\\VFPUU3B4\\rusher.html}
}

@inproceedings{saleemFEASIBLEFeatureBasedSPARQL2015,
  title = {{{FEASIBLE}}: {{A Feature-Based SPARQL Benchmark Generation Framework}}},
  shorttitle = {{{FEASIBLE}}},
  booktitle = {The {{Semantic Web}} - {{ISWC}} 2015},
  author = {Saleem, Muhammad and Mehmood, Qaiser and Ngonga Ngomo, Axel-Cyrille},
  editor = {Arenas, Marcelo and Corcho, Oscar and Simperl, Elena and Strohmaier, Markus and d' Aquin, Mathieu and Srinivas, Kavitha and Groth, Paul and Dumontier, Michel and Heflin, Jeff and Thirunarayan, Krishnaprasad and Thirunarayan, Krishnaprasad and Staab, Steffen},
  options = {useprefix=true},
  date = {2015},
  series = {Lecture {{Notes}} in {{Computer Science}}},
  pages = {52--69},
  publisher = {{Springer International Publishing}},
  location = {{Cham}},
  doi = {10.1007/978-3-319-25007-6_4},
  url = {https://link.springer.com/chapter/10.1007/978-3-319-25007-6_4},
  abstract = {Benchmarking is indispensable when aiming to assess technologies with respect to their suitability for given tasks. While several benchmarks and benchmark generation frameworks have been developed to evaluate triple stores, they mostly provide a one-fits-all solution to the benchmarking problem. This approach to benchmarking is however unsuitable to evaluate the performance of a triple store for a given application with particular requirements. We address this drawback by presenting FEASIBLE, an automatic approach for the generation of benchmarks out of the query history of applications, i.e., query logs. The generation is achieved by selecting prototypical queries of a user-defined size from the input set of queries. We evaluate our approach on two query logs and show that the benchmarks it generates are accurate approximations of the input query logs. Moreover, we compare four different triple stores with benchmarks generated using our approach and show that they behave differently based on the data they contain and the types of queries posed. Our results suggest that FEASIBLE generates better sample queries than the state of the art. In addition, the better query selection and the larger set of query types used lead to triple store rankings which partly differ from the rankings generated by previous works.},
  isbn = {978-3-319-25007-6},
  langid = {english},
  keywords = {Composite Error,Query Feature,SPARQL Query,Triple Pattern,Triple Store},
  file = {D\:\\Zotero\\storage\\KJDXYBJ3\\Saleem et al. - 2015 - FEASIBLE A Feature-Based SPARQL Benchmark Generat.pdf}
}

@inproceedings{saleemHowRepresentativeSPARQL2019,
  title = {How {{Representative Is}} a {{SPARQL Benchmark}}? {{An Analysis}} of {{RDF Triplestore Benchmarks}}},
  shorttitle = {How {{Representative Is}} a {{SPARQL Benchmark}}?},
  booktitle = {The {{World Wide Web Conference}}},
  author = {Saleem, Muhammad and Szárnyas, Gábor and Conrads, Felix and Bukhari, Syed Ahmad Chan and Mehmood, Qaiser and Ngonga Ngomo, Axel-Cyrille},
  date = {2019-05-13},
  pages = {1623--1633},
  publisher = {{ACM}},
  location = {{San Francisco CA USA}},
  doi = {10.1145/3308558.3313556},
  url = {https://dl.acm.org/doi/10.1145/3308558.3313556},
  urldate = {2021-11-22},
  abstract = {Triplestores are data management systems for storing and querying RDF data. Over recent years, various benchmarks have been proposed to assess the performance of triplestores across different performance measures. However, choosing the most suitable benchmark for evaluating triplestores in practical settings is not a trivial task. This is because triplestores experience varying workloads when deployed in real applications. We address the problem of determining an appropriate benchmark for a given real-life workload by providing a fine-grained comparative analysis of existing triplestore benchmarks. In particular, we analyze the data and queries provided with the existing triplestore benchmarks in addition to several real-world datasets. Furthermore, we measure the correlation between the query execution time and various SPARQL query features and rank those features based on their significance levels. Our experiments reveal several interesting insights about the design of such benchmarks. With this fine-grained evaluation, we aim to support the design and implementation of more diverse benchmarks. Application developers can use our result to analyze their data and queries and choose a data management system.},
  eventtitle = {{{WWW}} '19: {{The Web Conference}}},
  isbn = {978-1-4503-6674-8},
  langid = {english},
  file = {D\:\\Zotero\\storage\\MLXVXXMQ\\Saleem et al. - 2019 - How Representative Is a SPARQL Benchmark An Analy.pdf}
}

@inproceedings{saleemLSQLinkedSPARQL2015,
  title = {{{LSQ}}: {{The Linked SPARQL Queries Dataset}}},
  shorttitle = {{{LSQ}}},
  booktitle = {The {{Semantic Web}} - {{ISWC}} 2015},
  author = {Saleem, Muhammad and Ali, Muhammad Intizar and Hogan, Aidan and Mehmood, Qaiser and Ngomo, Axel-Cyrille Ngonga},
  editor = {Arenas, Marcelo and Corcho, Oscar and Simperl, Elena and Strohmaier, Markus and d' Aquin, Mathieu and Srinivas, Kavitha and Groth, Paul and Dumontier, Michel and Heflin, Jeff and Thirunarayan, Krishnaprasad and Staab, Steffen},
  options = {useprefix=true},
  date = {2015},
  series = {Lecture {{Notes}} in {{Computer Science}}},
  pages = {261--269},
  publisher = {{Springer International Publishing}},
  location = {{Cham}},
  doi = {10.1007/978-3-319-25010-6_15},
  abstract = {We present LSQ: a Linked Dataset describing SPARQL queries extracted from the logs of public SPARQL endpoints. We argue that LSQ has a variety of uses for the SPARQL research community, be it for example to generate custom benchmarks or conduct analyses of SPARQL adoption. We introduce the LSQ data model used to describe SPARQL query executions as RDF. We then provide details on the four SPARQL endpoint logs that we have RDFised thus far. The resulting dataset contains 73 million triples describing 5.7 million query executions.},
  isbn = {978-3-319-25010-6},
  langid = {english},
  keywords = {British Museum,Lorenz Curve,Outgoing Link,Query Execution,SPARQL Query},
  file = {D\:\\Zotero\\storage\\HZRB2H2A\\Saleem et al. - 2015 - LSQ The Linked SPARQL Queries Dataset.pdf}
}

@unpublished{schmidtSP2BenchSPARQLPerformance2008,
  title = {{{SP2Bench}}: {{A SPARQL Performance Benchmark}}},
  shorttitle = {{{SP2Bench}}},
  author = {Schmidt, Michael and Hornung, Thomas and Lausen, Georg and Pinkel, Christoph},
  date = {2008-10-21},
  eprint = {0806.4627},
  eprinttype = {arxiv},
  primaryclass = {cs},
  url = {http://arxiv.org/abs/0806.4627},
  urldate = {2021-11-22},
  abstract = {Recently, the SPARQL query language for RDF has reached the W3C recommendation status. In response to this emerging standard, the database community is currently exploring efficient storage techniques for RDF data and evaluation strategies for SPARQL queries. A meaningful analysis and comparison of these approaches necessitates a comprehensive and universal benchmark platform. To this end, we have developed SP2Bench, a publicly available, language-specific SPARQL performance benchmark. SP2Bench is settled in the DBLP scenario and comprises both a data generator for creating arbitrarily large DBLP-like documents and a set of carefully designed benchmark queries. The generated documents mirror key characteristics and social-world distributions encountered in the original DBLP data set, while the queries implement meaningful requests on top of this data, covering a variety of SPARQL operator constellations and RDF access patterns. As a proof of concept, we apply SP2Bench to existing engines and discuss their strengths and weaknesses that follow immediately from the benchmark results.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Databases,Computer Science - Performance},
  file = {D\:\\Zotero\\storage\\S4T6ASV8\\Schmidt et al. - 2008 - SP2Bench A SPARQL Performance Benchmark.pdf}
}

@article{voigtAnotherTripleStore2012,
  title = {Yet {{Another Triple Store Benchmark}}? {{Practical Experiences}} with {{Real-World Data}}},
  author = {Voigt, Martin and Mitschick, Annett and Schulz, Jonas},
  date = {2012},
  pages = {10},
  abstract = {Although quite a number of RDF triple store benchmarks have already been conducted and published, it appears to be not that easy to find the right storage solution for your particular Semantic Web project. A basic reason is the lack of comprehensive performance tests with real-world data. Confronted with this problem, we setup and ran our own tests with a selection of four up-to-date triple store implementations – and came to interesting findings. In this paper, we briefly present the benchmark setup including the store configuration, the datasets, and the test queries. Based on a set of metrics, our results demonstrate the importance of real-world datasets in identifying anomalies or differences in reasoning. Finally, we must state that it is indeed difficult to give a general recommendation as no store wins in every field.},
  langid = {english},
  file = {D\:\\Zotero\\storage\\FCTM47FF\\Voigt et al. - 2012 - Yet Another Triple Store Benchmark Practical Expe.pdf}
}

@inproceedings{vrandecicWikidataNewPlatform2012,
  title = {Wikidata: A New Platform for Collaborative Data Collection},
  shorttitle = {Wikidata},
  booktitle = {Proceedings of the 21st {{International Conference}} on {{World Wide Web}}},
  author = {Vrandečić, Denny},
  date = {2012-04-16},
  series = {{{WWW}} '12 {{Companion}}},
  pages = {1063--1064},
  publisher = {{Association for Computing Machinery}},
  location = {{New York, NY, USA}},
  doi = {10.1145/2187980.2188242},
  url = {https://doi.org/10.1145/2187980.2188242},
  urldate = {2021-11-29},
  abstract = {This year, Wikimedia starts to build a new platform for the collaborative acquisition and maintenance of structured data: Wikidata. Wikidata's prime purpose is to be used within the other Wikimedia projects, like Wikipedia, to provide well-maintained, high-quality data. The nature and requirements of the Wikimedia projects require to develop a few novel, or at least unusual features for Wikidata: Wikidata will be a secondary database, i.e. instead of containing facts it will contain references for facts. It will be fully internationalized. It will contain inconsistent and contradictory facts, in order to represent the diversity of knowledge about a given entity.},
  isbn = {978-1-4503-1230-1},
  keywords = {linked data,semantic web,wikidata,wikipedia},
  file = {D\:\\Zotero\\storage\\I735Q8PK\\Vrandečić - 2012 - Wikidata a new platform for collaborative data co.pdf}
}

@article{wilkinsonFAIRGuidingPrinciples2016,
  title = {The {{FAIR Guiding Principles}} for Scientific Data Management and Stewardship},
  author = {Wilkinson, Mark D. and Dumontier, Michel and Aalbersberg, IJsbrand Jan and Appleton, Gabrielle and Axton, Myles and Baak, Arie and Blomberg, Niklas and Boiten, Jan-Willem and da Silva Santos, Luiz Bonino and Bourne, Philip E. and Bouwman, Jildau and Brookes, Anthony J. and Clark, Tim and Crosas, Mercè and Dillo, Ingrid and Dumon, Olivier and Edmunds, Scott and Evelo, Chris T. and Finkers, Richard and Gonzalez-Beltran, Alejandra and Gray, Alasdair J. G. and Groth, Paul and Goble, Carole and Grethe, Jeffrey S. and Heringa, Jaap and ’t Hoen, Peter A. C. and Hooft, Rob and Kuhn, Tobias and Kok, Ruben and Kok, Joost and Lusher, Scott J. and Martone, Maryann E. and Mons, Albert and Packer, Abel L. and Persson, Bengt and Rocca-Serra, Philippe and Roos, Marco and van Schaik, Rene and Sansone, Susanna-Assunta and Schultes, Erik and Sengstag, Thierry and Slater, Ted and Strawn, George and Swertz, Morris A. and Thompson, Mark and van der Lei, Johan and van Mulligen, Erik and Velterop, Jan and Waagmeester, Andra and Wittenburg, Peter and Wolstencroft, Katherine and Zhao, Jun and Mons, Barend},
  options = {useprefix=true},
  date = {2016-03-15},
  journaltitle = {Scientific Data},
  shortjournal = {Sci Data},
  volume = {3},
  number = {1},
  pages = {160018},
  publisher = {{Nature Publishing Group}},
  issn = {2052-4463},
  doi = {10.1038/sdata.2016.18},
  url = {https://www.nature.com/articles/sdata201618},
  urldate = {2022-01-31},
  abstract = {There is an urgent need to improve the infrastructure supporting the reuse of scholarly data. A diverse set of stakeholders—representing academia, industry, funding agencies, and scholarly publishers—have come together to design and jointly endorse a concise and measureable set of principles that we refer to as the FAIR Data Principles. The intent is that these may act as a guideline for those wishing to enhance the reusability of their data holdings. Distinct from peer initiatives that focus on the human scholar, the FAIR Principles put specific emphasis on enhancing the ability of machines to automatically find and use the data, in addition to supporting its reuse by individuals. This Comment is the first formal publication of the FAIR Principles, and includes the rationale behind them, and some exemplar implementations in the community.},
  issue = {1},
  langid = {english},
  keywords = {Publication characteristics,Research data},
  file = {D\:\\Zotero\\storage\\HAE46VIX\\Wilkinson et al. - 2016 - The FAIR Guiding Principles for scientific data ma.pdf;D\:\\Zotero\\storage\\FLFPSFXV\\sdata201618.html}
}

@preamble{ "\ifdefined\DeclarePrefChars\DeclarePrefChars{'’-}\else\fi " }

