\chapter{Related Work}
\label{ch:related_work}
%%%%%%%%%%%%%%%%%%
% - description of existing techniques and approaches
%%%%%%%%%%%%%%%%%%

This chapter reviews the state of the art of \ts{} benchmarking.\\

Several benchmarks have been proposed and developed to test \tsp{} \cite{alucDiversifiedStressTesting2014, guoLUBMBenchmarkOWL2005, morseyDBpediaSPARQLBenchmark2011, saleemFEASIBLEFeatureBasedSPARQL2015, schmidtSP2BenchSPARQLPerformance2008}.
Many of these existing benchmarks focus on different goals and scenarios.
Section \ref{sec:synthetic_benchmarks} and \ref{sec:benchmarks_real_data} explain the different benchmark types used to benchmark \tsp{}.
Section \ref{sec:benchmark_frameworks} gives a short introduction to benchmark execution frameworks.
An introduction to benchmarking, in general, is given in section \ref{sec:benchmark}.

\section{Synthetic Benchmarks}
\label{sec:synthetic_benchmarks}
Synthetic benchmarks are benchmarks where the data is artificially generated.
Often the generation is influenced by real-world scenarios to generate data comparable to real-world datasets \cite{guoLUBMBenchmarkOWL2005}.
These synthetic benchmarks have the offer the benefit of being able to be generated to arbitrary sizes.
The main criticism for synthetic benchmarks is that the generated data can quickly become skewed and repetitive.
Often the generated scenarios are criticized for not being representative enough of a real-world scenario \cite{saleemFEASIBLEFeatureBasedSPARQL2015}.
In the following paragraphs, we introduce three benchmarks that are often mentioned when considering synthetic \ts{} benchmarks.
\\

The \ac{lubm} Benchmark \cite{guoLUBMBenchmarkOWL2005} is a synthetic benchmark which focuses on the reasoning and inferencing capabilities of the \tsp{} under test.
The test data is located in the university domain and can be generated to arbitrary size.
Fourteen extensional queries are provided that represent and test a variety of properties.

Another synthetic benchmark is \ac{sp2bench} \cite{schmidtSP2BenchSPARQLPerformance2008}.
The data generated stems from the DBLP scenario. 
During the generation process, the generated key characteristics and word distributions are chosen to match the distributions of the original DBLP dataset.
The provided queries are primarily complex, and the mean size of the result sets is above one million \cite{saleemFEASIBLEFeatureBasedSPARQL2015}.
They also test for SPARQL features like union and optional graph patterns.

The \ac{watdiv} generates a synthetic benchmark and consists of multiple tools \cite{alucDiversifiedStressTesting2014}.
The first tool is the data generator that generates scalable and customizable datasets based on the \ac{watdiv} data model schema.
The query template generator generates diverse query templates, which will then be used to generate actual queries.
Lastly, the queries are generated with the query generator, which instantiates the templates with actual \ac{rdf} terms from the generated dataset.
For each template, multiple queries can be generated.
The benchmark only focuses on SELECT queries that do not use the union and optional pattern features of SPARQL.

\section{Real-World Benchmarks}
\label{sec:benchmarks_real_data}
Real-world benchmarks use real datasets that were used in productive settings or are still being used.
The actual queries are often taken from query logs of \tsp{}, and the datasets are based on real datasets \cite{morseyDBpediaSPARQLBenchmark2011, saleemFEASIBLEFeatureBasedSPARQL2015}.

FEASIBLE is a benchmark generation framework that generates queries from provided query logs \cite{saleemFEASIBLEFeatureBasedSPARQL2015}.
This has the advantage that the data used for the benchmark could stem from queries about a specialized real-world topic rather than an abstract synthetic model.
FEASIBLE can also generate queries for the other SPARQL query types besides SELECT.

\section{Benchmark Execution Frameworks}
\label{sec:benchmark_frameworks}
% Most benchmarks come with their own execution framework \todo{cite!}
As the name suggests, benchmark execution frameworks help in the execution of database benchmarks.
Their tasks are to load the data, execute the test queries and measure the defined metrics to evaluate the system under test.

Many benchmarks provide their own execution environments, which makes comparing between benchmarks difficult.
Often those environments are specialized for the given benchmark and are not easily interchangeable \cite{conradsIguanaGenericFramework2017}.

The following sections focus on benchmark-independent execution frameworks.


\subsection{IGUANA}
\label{sec:iguana}
\iguana{} is a SPARQL, benchmark-independent execution framework \cite{conradsIguanaGenericFramework2017}.
The framework gets a dataset and a set of queries and operations as input and then uses the SPARQL endpoint of the \ts{} to load and update the data, as well as to perform the benchmark queries.
It allows the measurement of the performance during loading and updating of data as well as parallel requests to the \ts{}.
\iguana{} is independent of any benchmarks, which allow it to run in different configurations and with various existing benchmarks and datasets.
This includes synthetic benchmarks (\ref{sec:synthetic_benchmarks}) and benchmarks based on real data (\ref{sec:benchmarks_real_data}).
The benchmark process is highly configurable by passing a configuration file to the \iguana{} framework.


\subsection{HOBBIT Framework}
The \textsc{Hobbit} framework is a distributed benchmarking platform designed to be able to scale up benchmarking for big linked-data applications \cite{roderHOBBITPlatformBenchmarking}.
It is an extensive framework that needs to be deployed on a local cluster or online computing services like Azure\footnote{\url{https://azure.microsoft.com/}} or AWS\footnote{\url{https://aws.amazon.com/}}.
The platform's deployment and deploying new benchmarks to the platform can be challenging for new users of the system \cite{roderHOBBITPlatformBenchmarking}.
The data for benchmarks has to be stored in docker containers or it needs to be generated or downloaded before a benchmark, which increases the complexity of the system.
The data is then sent via message queues to the benchmarked system.
\\

With the Basilisk platform, we developed a specialized solution for continuous benchmarking of \tsp{} which does not need the technical complexity present in the \textsc{Hobbit} framework.
Implementing this functionality into the \textsc{Hobbit} framework would introduce another level of complexity to the HOBBIT system.
Basilisk focuses on a smaller use-case of benchmarking SPARQL endpoints continuously with as little overhead as possible.

