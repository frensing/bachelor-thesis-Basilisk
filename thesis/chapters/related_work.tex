\chapter{Related Work}
\label{ch:related_work}
%%%%%%%%%%%%%%%%%%
% - description of existing techniques and approaches
%%%%%%%%%%%%%%%%%%

This chapter reviews the state of the art of \ts{} benchmarking.\\

Several benchmarks have been proposed and developed to test \tsp{} \cite{alucDiversifiedStressTesting2014, guoLUBMBenchmarkOWL2005, morseyDBpediaSPARQLBenchmark2011, saleemFEASIBLEFeatureBasedSPARQL2015, schmidtSP2BenchSPARQLPerformance2008}.
Many of these existing benchmarks focus on different goals and scenarios.
Section \ref{sec:synthetic_benchmarks} and \ref{sec:benchmarks_real_data} explain the different benchmark types used to benchmark \tsp{}.
Section \ref{sec:benchmark_frameworks} gives a short introduction to benchmark execution frameworks.
An introduction to benchmarking in general is given in section \ref{sec:benchmark}.

\section{Synthetic Benchmarks}
\label{sec:synthetic_benchmarks}
Synthetic benchmarks are benchmarks where the data is artificially generated.
Often the generation is influenced by real world scenarios to generate data comparable to real world datasets \cite{guoLUBMBenchmarkOWL2005}.
These synthetic benchmarks have the advantage, that they can be generated to arbitrary sizes.
The main point of criticism for synthetic benchmarks is that the generated data can easily become too abstract.
Often the generated scenarios are criticized for not being representative of a real world scenario \cite{saleemFEASIBLEFeatureBasedSPARQL2015}.
In the following paragraphs we introduce three benchmarks that are often mentioned when considering synthetic \ts{} benchmarks.
\\

The LUBM Benchmark \cite{guoLUBMBenchmarkOWL2005} is a synthetic benchmark which focuses on the reasoning and inferencing capabilities of the \tsp{} under test.
The test data is located in the university domain and can be generated to arbitrary size.
Fourteen extensional queries are provided that represent and test a variety of properties.

Another synthetic benchmark is SPÂ²Bench \cite{schmidtSP2BenchSPARQLPerformance2008}.
The data generated stems from the DBLP scenario. 
During the generation process, the generated key characteristics and word distributions are chosen to match the distributions of the original DBLP dataset.
The provided queries are mostly complex and the mean size of the result sets is above one million \cite{saleemFEASIBLEFeatureBasedSPARQL2015}.
They also test for SPARQL features like union and optional graph patterns.

The WatDiv suite generates a synthetic benchmarks and consists of multiple tools \cite{alucDiversifiedStressTesting2014}.
First tool is the data generator which generates scalable and customizable datasets based on the WatDiv data model schema.
The query template generator generates diverse query templates which will then be used to generate actual queries.
The queries get generated with the query generator which instantiates the templates with actual RDF terms from the generated dataset.
For each template multiple queries can be generated.
The benchmark only focuses on SELECT queries that does not make use of the union and optional pattern features of SPARQL.

\section{Benchmarks Using Real Data}
\label{sec:benchmarks_real_data}
Benchmarks using real data are benchmarks for which copies of real datasets and queries are used.
The real queries are often taken from query logs of \tsp{} and the datasets are based on real datasets \cite{morseyDBpediaSPARQLBenchmark2011, saleemFEASIBLEFeatureBasedSPARQL2015}.

FEASIBLE is a benchmark generation framework which generates datasets and queries from provided query logs \cite{saleemFEASIBLEFeatureBasedSPARQL2015}.
This has the advantage that the data used for the benchmark could stem from queries about a specialized real world topics rather than an abstract synthetic model.
FEASIBLE can also generate queries for the other SPARQL query types beside SELECT.

\section{Benchmark Execution Frameworks}
\label{sec:benchmark_frameworks}
% Most benchmarks come with their own execution framework \todo{cite!}
Benchmark execution frameworks, as the name suggests, help in the execution of database benchmarks.
Their tasks are to load the data, execute the test queries and measure the defined metrics to evaluate the system under test.

Many benchmarks provide their own execution environments, which makes the comparison between benchmarks difficult.
Often those environments are specialized for the given benchmark and are not easily interchangeable \cite{conradsIguanaGenericFramework2017}.

The next sections focus on benchmark-independent execution frameworks.


\subsection{IGUANA}
\label{sec:iguana}
\iguana{} is a SPARQL, benchmark-independent execution framework \cite{conradsIguanaGenericFramework2017}.
The framework gets a dataset and a set of queries and operations as input and then uses the SPARQL endpoint of the \ts{} to load and update the data, as well as to perform the benchmark queries.
It allows the measurement of the performance during loading and updating of data as well as parallel requests to the \ts{}.
\iguana{} is independent of any benchmarks which allows it to run in different configurations and with various existing benchmarks and datasets.
This includes synthetic benchmarks (\ref{sec:synthetic_benchmarks}) and benchmarks based on real data (\ref{sec:benchmarks_real_data}).
The benchmark process is highly configurable by passing a configuration file to the \iguana{} framework.


\subsection{HOBBIT Framework}
The \textsc{Hobbit} framework is a distributed benchmarking platform designed to be able to scale up benchmarking for big linked-data applications \cite{roderHOBBITPlatformBenchmarking}.
It is a big framework which needs to be deployed on a local cluster or online computing services like Azure\footnote{\url{https://azure.microsoft.com/}} or AWS\footnote{\url{https://aws.amazon.com/}}.
The deployment of the platform and deploying new benchmarks to the platform can be challenging for new users of the system \cite{roderHOBBITPlatformBenchmarking}.
The data for benchmarks has to be stored in docker containers or it needs to be generated or downloaded before a benchmark, which increases the complexity of the system.
The data is then sent over message queues to the benchmarked system.
\\

With the Basilisk platform we try to develop a specialized solution for continuous benchmarking of \tsp{} which does not need the technical complexity present in the \textsc{Hobbit} framework.
Implementing this functionality into the \textsc{Hobbit} framework would introduce another level of complexity to the system.
Basilisk focuses on a smaller use-case of benchmarking SPARQL endpoints continuously with as little overhead as possible.

