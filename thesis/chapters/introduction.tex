\chapter{Introduction}
\label{ch:introduction}
%%%%%%%%%%%%%%%%%%%
% - description of the topic and the context in which it is inserted
% - topic motivation

% “[X exists]. [It’s based on Y]. [Y is slow]. [Z is more efficient than Y]. [Want to replace Y with Z]”
%%%%%%%%%%%%%%%%%%%

%\todo[inline]{introduce ts acronym}


In the field of Semantic Web, knowledge graphs are the central structure to represent data and its relationships.
To easily store and query the data in knowledge graphs, some specialized knowledge graph databases are required.
The special kind of database developed to store knowledge graphs are called \tsp{}. \\

Since knowledge graphs can contain huge amounts of data of various structure which can also be subject to many changes, \tsp{} need to be able to handle many different workloads.
Some scenarios need to handle huge amounts of data being added, while others need to handle a lot of changes on the current data.
\todo[inline]{more example for things that can be different, e.g. simple ontologies vs complex}
To better test and compare \tsp{} in these diverse scenarios, benchmarks are performed to allow an appropriate comparison between different \tsp{} \cite{saleemHowRepresentativeSPARQL2019}.

In general, benchmarks are used to measure and compare the performance of computer programs and systems with a defined set of operations.
Often they are designed to mimic and reproduce a particular type of workload to the system \cite{saleemFEASIBLEFeatureBasedSPARQL2015, morseyDBpediaSPARQLBenchmark2011}.
In the context of \tsp{}, a benchmark consists of creating a given knowledge graph on which multiple queries and operations are performed \cite{conradsIguanaGenericFramework2017}.

Usually \tsp{} are developed in long iterations and are benchmarked only in a late stage of such a development iteration.
Benchmarks and the evaluation of their results are often done manually and bind time of developers.
Thus, performance regressions are  found very late or never.


Several benchmarks for \tsp{} have been proposed \cite{saleemHowRepresentativeSPARQL2019}.
Most benchmarks are executed in their own execution environment.
To better compare different benchmarks, benchmark-independent frameworks have been developed.
\iguana{} is such a benchmark-independent execution framework \cite{conradsIguanaGenericFramework2017}, that can be used to measure the performance of \tsp{} under several parallel query request.
Currently the benchmark execution framework needs to be configured and started manually for every new benchmark configuration.

Basilisk\footnote{\url{https://github.com/dice-group/Basilisk}} is a continuous benchmarking service for \tsp{} which internally uses \iguana{} to perform the benchmarks.
The idea is that the Basilisk service will check automatically (continuously) for new versions of a given \tsp{} and starts benchmarks with the \iguana{} framework.
Further, it should be possible to start custom benchmarks on demand.
If a new version is found in a provided \gh{} or \dockh{} repository, Basilisk should automatically setup a benchmark environment and start a benchmarking suite.

\todo[inline]{continue paragraph}
This means that developers do not have to worry about performing benchmarks at different stages of development.

\todo[inline]{it not clear yet that basilisk was there before. 
	also it would be better to move the notion of the previous code basis above the Basilisk is a cont... paragraph (or at its start).}
In this thesis, we continue the development of the Basilisk platform and deploy an instance to a publicly available virtual machine.
\\

The thesis is structured as follows:
In Chapter \ref{ch:related_work}, we take a look at the state of the art of \ts{} benchmarking. 
Chapter \ref{ch:background} introduces the fundamental concepts and topics necessary for understanding this thesis.
This consists of topics from the field of Semantic Web as well as topics from the field of software development.
In chapter \ref{ch:approach}, we describe and review the architecture used in the Basilisk platform.
Chapter \ref{ch:implementation} presents the development and implementations that are performed to finalize the platform.
Finally, in chapter \ref{ch:evaluation} we evaluate the platform and its provided benefits to the \ts{} benchmark process.

