\chapter{Introduction}
\label{ch:introduction}
%%%%%%%%%%%%%%%%%%%
% - description of the topic and the context in which it is inserted
% - topic motivation

% “[X exists]. [It’s based on Y]. [Y is slow]. [Z is more efficient than Y]. [Want to replace Y with Z]”
%%%%%%%%%%%%%%%%%%%

\todo[inline]{introduce ts acronym}


In the field of Semantic Web, knowledge graphs are an important structure to represent data and its relationships.
To easily store and query the data in these knowledge graphs, some data structure or database is needed.
The special kind of database developed to store knowledge graphs are called \tsp{}. \\

Since knowledge graphs can contain huge amounts of data which can also be subject to many changes, \tsp{} need to be able to handle many different workloads.
Some scenarios need to handle huge amount of data being added, while others need to handle a lot of changes on the current data.
To better test and compare \tsp{} in these diverse scenarios, benchmarks are performed to allow an appropriate comparison between different \tsp{}\cite{saleemHowRepresentativeSPARQL2019}.

In general, Benchmarks are used to measure and compare the performance of computer programs and systems with a defined set of operations.
Often they are designed to mimic and reproduce a particular type of workload to the system.
In the context of \tsp{}, a benchmark usually consists of creating a given knowledge graph on which multiple queries and operations are performed.

Often \tsp{} are developed in long iterations and are bench-marked only in a late stage of such an development iteration.
Today benchmarks and the evaluation of their results are usually done manually and bind developers time.
Thus, performance regressions are  found very late or never.


Several benchmarks for \tsp{} have been proposed \cite{saleemHowRepresentativeSPARQL2019}.
\iguana{} is a benchmark-independent execution framework \cite{conradsIguanaGenericFramework2017} that can measure the performance of \tsp{} under several parallel query request.
Currently the benchmark execution framework needs to be installed and benchmarks need to be started manually.
Basilisk is a continuous benchmarking service for \tsp{} which internally uses \iguana{} to perform the benchmarks.
The idea is that the Basilisk service will check automatically (continuously) for new versions of \tsp{} and start benchmarks with the \iguana{} framework.
Further it should be possible to start custom benchmarks on demand.
If a new version is found in a provided GitHub- or Docker Hub-repository, Basilisk will automatically setup a benchmark environment and starts a benchmarking suite.

This means that developers do not have to worry about performing benchmarks at different stages of development.

In this thesis we continue the development of the Basilisk platform and deploy an instance to a publicly available virtual machine.
\\

The thesis is structured as follows. 
In Chapter \ref{ch:related_work} we take a look at the state of the art of \ts{} benchmarking. 
Chapter \ref{ch:background} introduces the fundamental concepts and topics to understand this thesis.
The chapter \ref{ch:approach} describes the architecture use in the Basilisk platform.

